---
Subject: "[[Indice - Information Retrieval|IR]]"
Super Topic: "[[Evaluation]]"
tags:
  - IR
  - Evaluation
  - SearchEngine
  - Online
Creation: 2024-10-20
---
Text REtrieval Conference (TREC), had the aim to create an evaluation infrastructure required for large-scale testing of IR technology. This includes research on *best methods for evaluation* as well as *development of the evaluation materials* themselves.


*Retrieval technology* is broadly interpreted to include a variety of techniques that enable and/or facilitate access to information not specifically structured for machine use.

TREC was sponsored by NIST (National Institute of Standard and Technologies) and is organized around a set of focused areas called *tracks*. a track has a motivating use case, which is generally an abstraction of user task.

In TREC test collection consist of three parts :
1. a set of document 
2. a set of information needs (called *topics*)
3. relevance judgements, an indication of which documents should be retrieved in response to which topic

We call a *RUN* the result of a retrieval system executing a task on a test collection.

